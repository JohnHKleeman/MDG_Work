{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 0.19.1\n",
      "numpy: 1.12.1\n",
      "matplotlib: 2.0.2\n",
      "pandas: 0.20.3\n",
      "sklearn: 0.19.0\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print('scipy: {}'.format(scipy.__version__)) # numpy\n",
    "import numpy as np\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "# matplotlib\n",
    "import datetime\n",
    "\n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "print('matplotlib: {}'.format(matplotlib.__version__)) # pandas\n",
    "import pandas as pd\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "#scikit-learn\n",
    "import sklearn\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "import xlrd\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Lasso, Ridge  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection #might be model_selection <--- this is the new one\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import zipcode\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import vincenty\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('SuperZooData/Registration_Data/SuperZoo_2016_Registrants_No_Minors.csv', header = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('SuperZooData/Registration_Data/SuperZoo_2017_Registrants_No_Minors.csv', header = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotting = data['AddDate Date'].value_counts().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.index.sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(150, 150))\n",
    "plt.plot(plotting.index.sort_values(ascending=True), plotting)\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.sort_values('date', ascending=True)\n",
    "plt.plot(df['date'], df['count'])\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dups_with_dif_index_check(data):\n",
    "    dupes = {}                        # checks to see if duplicated columns with different index values\n",
    "    for col_a in data.columns:\n",
    "        dupes[col_a] = []\n",
    "\n",
    "        for col_b in data.columns:\n",
    "            if col_b in dupes.keys():\n",
    "                continue\n",
    "            if (data.loc[:,col_a] == data.loc[:,col_b]).sum() == len(data.loc[:,col_a]):\n",
    "                dupes[col_a].append(col_b)    \n",
    "   \n",
    "    bad_cols = []   \n",
    "    for col, dup_cols in dupes.iteritems():\n",
    "        bad_cols += dup_cols\n",
    "    bad_cols = set(bad_cols)\n",
    "\n",
    "    data = data.loc[:,~data.columns.isin(bad_cols)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_dups_na(data):\n",
    "    data = data.loc[:,~data.columns.duplicated()] \n",
    "    data = data.dropna(axis=1, how='all')\n",
    "    data = data.dropna(axis=0, how='all')\n",
    "    data = dups_with_dif_index_check(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def def_time_modify(data, date_checker):\n",
    "    data[\"AddDate Date\"] = pd.to_datetime(data[\"AddDate Date\"])\n",
    "    data['ModifyDate Date'] = pd.to_datetime(data['ModifyDate Date'])\n",
    "    for a in range(0,len(data.iloc[:,data.columns.get_loc('ModifyDate Date')])):\n",
    "        if data.iloc[a,data.columns.get_loc('ModifyDate Date')] > date_checker:\n",
    "            data.iloc[a,data.columns.get_loc('ModifyDate Date')] = data.iloc[a,data.columns.get_loc(\"AddDate Date\")]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time_update(data, date_checker):\n",
    "    data = data[data[\"AddDate Date\"] <= date_checker]\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_drop_other_text(data):\n",
    "    col = []\n",
    "    for a in data.columns:\n",
    "        if \"_text\" in a or \"_other\" in a:\n",
    "            col.append(a)\n",
    "    data=data.drop(labels = col, axis =1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rid_of_nonnumerical_answer(data):\n",
    "    col = [] # THIS NEEDS TO BE CHANGED FOR NEW DATA SET\n",
    "    checker = 1\n",
    "    for a in data.columns:\n",
    "        if '_codes' in a and checker ==1:\n",
    "            checker = 0\n",
    "        if '_codes' not in a and checker == 0:\n",
    "            col.append(a)\n",
    "    data = data.drop(col, axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def answers_dummies(data):\n",
    "    column =[]\n",
    "    for a in data.columns:\n",
    "        if \"_codes\" in a:\n",
    "            column.append(a)  \n",
    "    for a in column:\n",
    "        new_data = data[a].str.get_dummies(sep=',')\n",
    "        data = pd.concat([data, new_data], axis=1)\n",
    "        data = data.drop([a], axis = 1)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_change_string_drop_dups(data):\n",
    "    for a in data.columns:\n",
    "        if data[a].dtype== np.dtype('O'):\n",
    "            data[a] = data[a].astype('S32')\n",
    "    cols = list(data)    # THIS DROPS ALL COLUMNS THAT HAVE THE SAME VALUE\n",
    "    nunique = data.apply(pd.Series.nunique)\n",
    "    cols_to_drop = nunique[nunique == 1].index\n",
    "    data = data.drop(cols_to_drop, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def purchase_dummies(data):\n",
    "    new_data = data['PurchaseItems'].str.get_dummies(sep=',')\n",
    "    data = pd.concat([data, new_data], axis=1)\n",
    "    data = data.drop(['PurchaseItems'], axis = 1)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def state_dummies(data):\n",
    "    s = data.State.value_counts()\n",
    "    s = data.State.value_counts()[s > (8)].index #8\n",
    "    holder = pd.get_dummies(data.State)\n",
    "    holder_smaller = holder[s]\n",
    "    data = pd.concat([data, holder_smaller], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_zip(data):\n",
    "    data.loc[data.loc[:,'ZipPostal'].str.contains('-', na = False),'ZipPostal'] = data.loc[data.loc[:,'ZipPostal'].str.contains('-', na = False),'ZipPostal'].str.split('-',n = 1, expand = True)[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zip_distance(data):\n",
    "    data = check_zip(data)\n",
    "    geolocator = Nominatim()\n",
    "    vegas = zipcode.isequal('89178')\n",
    "    veg = (vegas.lat, vegas.lon)\n",
    "    col= []\n",
    "    for idx, val in enumerate (data.ZipPostal):\n",
    "        try:\n",
    "            city = zipcode.isequal(val)\n",
    "            cit = (city.lat, city.lon)\n",
    "            col.append(vincenty(veg, cit).miles)\n",
    "        except:\n",
    "            try:\n",
    "                city = geolocator.geocode(data.loc[idx,'Country']+\" \"+val)\n",
    "                cit = (city.latitude, city.longitude)\n",
    "                col.append(vincenty(veg, cit).miles)\n",
    "            except:\n",
    "                col.append('nan')\n",
    "    data.insert(0, 'Distance', col, allow_duplicates=False)\n",
    "   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def email_split_username(data):\n",
    "    data.loc[:,'Email_Username'] = data.Email.str.split('@',n = 1, expand = True)[0]\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def email_contains_username(data):\n",
    "    data.loc[:,'Email_Username']=~data.loc[:,'Email_Username'].str.contains('spa|wash|claw|pet|dog|woof|wag|tail|bark|groom|paw|animal|chow|poodle|fur|hound|canine|pup|pooch', regex = True) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_split_host(data):\n",
    "    data.loc[:,'Email_Host'] = data.Email.str.split('@',n = 1, expand = True)[1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_cleanup(data):\n",
    "    data.Email = data.Email.str.lower()\n",
    "    data.loc[:,'Email'][~data.loc[:,'Email'].str.contains('@')] = 'a@a'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_contains_hostname(data):\n",
    "    data.loc[data.loc[:,'Email_Host'].str.contains('spa|wash|claw|pet|dog|woof|wag|tail|bark|groom|paw|animal|chow|poodle|fur|hound|canine|pup|pooch', na = False,regex = True), 'Email_Host'] = 'PET'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_change(data):\n",
    "    data = email_cleanup(data)\n",
    "    data = email_split_username(data)\n",
    "    data = email_split_host(data)\n",
    "    data = email_contains_username(data)\n",
    "    data = email_contains_hostname(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def email_dummies(data):\n",
    "    data = email_change(data)\n",
    "    s = data.Email_Host.value_counts()\n",
    "    s = data.Email_Host.value_counts()[s > (48)].index  #48\n",
    "    holder = pd.get_dummies(data.Email_Host)\n",
    "    holder_smaller = holder[s]\n",
    "    data = pd.concat([data, holder_smaller], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def column_drops(data):\n",
    "    data = data.drop(['WebUrl',\"CheckInDate Date\",\"Aux1\", \"Aux5\",\"FirstPrintDate Time\",\"FirstPrintLocId\", 'FirstPrintStationId','CheckInDate Time',\"ModifyDate Time\", \"IndexDate\"], axis = 1)\n",
    "    data = data.drop(['RegTypeDesc','SourceApp','CountryName','StreamSteps','StreamName', 'RegType','StreamStep',\"AddUserId\",\"ModifyUserId\",\"AddDate Time\"], axis = 1)\n",
    "    data = data.drop([\"Pwd\", \"UserName\", \"AccountKeyPublic\",\"AccountKeyPrivate\", \"Notes\",'Id', 'AccountId', 'ClientIp', 'StreamId','State', 'StreamStatus'], axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_dummies(data):\n",
    "    data = pd.get_dummies(data,columns = ['Aux6', 'ApprovalUserId', 'LangId', 'MemberStatus', 'TermsAccepted'], dummy_na = True)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_dummies_cleanup(data):\n",
    "    d = {'3001': ['3001', '3001.1']}   \n",
    "    data = data.rename(columns=lambda c: d[c].pop(0) if c in d.keys() else c)\n",
    "    d = {'2001': ['2001', '2001.1']}\n",
    "    data = data.rename(columns=lambda c: d[c].pop(0) if c in d.keys() else c)\n",
    "    # renames duplicate column names\n",
    "    d = {'TermsAccepted_nan': ['TermsAccepted_nan', 'TermsAccepted_nan.1']}\n",
    "    data = data.rename(columns=lambda c: d[c].pop(0) if c in d.keys() else c)\n",
    "    d = {'MemberStatus_nan': ['MemberStatus_nan', 'MemberStatus_nan.1']}\n",
    "    data = data.rename(columns=lambda c: d[c].pop(0) if c in d.keys() else c)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(data):\n",
    "    date_checker = datetime.datetime(2016,7,23) #train_X 2016,6,3 2016,7,23    test_X 2017,5,24  2017,7,13   \n",
    "    data = zip_distance(data)\n",
    "    data = data.apply(pd.to_numeric, errors='ignore')\n",
    "    data = clear_dups_na(data)\n",
    "    data = def_time_modify(data, date_checker)\n",
    "    data = date_time_update(data, date_checker)                      #DONT FORGET TO CHANGE THIS UP HERE! THE DATES!!!\n",
    "    data = data_drop_other_text(data)\n",
    "    data = get_rid_of_nonnumerical_answer(data)\n",
    "    data = answers_dummies(data)\n",
    "    data = data_change_string_drop_dups(data)\n",
    "    data = purchase_dummies(data)\n",
    "    data = state_dummies(data)\n",
    "    data = email_dummies(data)\n",
    "    data = column_drops(data)\n",
    "    data = data_dummies(data)\n",
    "    data = data_dummies_cleanup(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = processing_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan              669\n",
       "8311.53649198    403\n",
       "238.142872527    121\n",
       "8929.99114339    102\n",
       "24.1011894369     85\n",
       "12.6881755978     70\n",
       "13.5118945605     68\n",
       "619.316904052     66\n",
       "22.6114343371     66\n",
       "11.0882285693     64\n",
       "11.2060057264     64\n",
       "16.8850855957     62\n",
       "2250.19822755     57\n",
       "629.658024429     55\n",
       "22.8625825643     54\n",
       "29.3606737306     53\n",
       "240.474544128     51\n",
       "9539.50296971     48\n",
       "630.537340659     46\n",
       "223.265620559     45\n",
       "16.3510640211     45\n",
       "0.0               44\n",
       "24.0282595168     44\n",
       "196.354932766     43\n",
       "616.018464698     42\n",
       "217.933598322     41\n",
       "19.9885974154     38\n",
       "87.1691187086     38\n",
       "22.4046895685     38\n",
       "20.3752471783     37\n",
       "                ... \n",
       "1518.81366811      1\n",
       "3225.80141754      1\n",
       "125.500131602      1\n",
       "2006.16510934      1\n",
       "1684.11413345      1\n",
       "2005.39380805      1\n",
       "218.298666094      1\n",
       "268.77899304       1\n",
       "454.600557328      1\n",
       "19.5071858454      1\n",
       "1068.37669541      1\n",
       "876.175377633      1\n",
       "341.292033843      1\n",
       "1065.54946494      1\n",
       "2395.85369586      1\n",
       "363.60732741       1\n",
       "1714.49902905      1\n",
       "250.53249888       1\n",
       "1388.14495312      1\n",
       "290.424362772      1\n",
       "1184.54084183      1\n",
       "1944.64302968      1\n",
       "328.096346832      1\n",
       "2250.05162019      1\n",
       "1985.45099833      1\n",
       "2172.67128184      1\n",
       "1061.32432108      1\n",
       "391.332224225      1\n",
       "256.899666923      1\n",
       "357.091913587      1\n",
       "Name: Distance, Length: 1825, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.Distance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = processing_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-12e440cc421c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data_Zoo.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.to_pickle(\"Data_Zoo.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data.to_pickle(\"Data_Zoo_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"Data_Zoo.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle(\"Data_Zoo_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12875, 357)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10242, 349)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(data.T.drop_duplicates(subset=data.index)).T  # to big of a dataset to use this to get rid of duplicate columns with different names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for a in range(0,len(data.columns)):                                          # look at values of columns\n",
    "#    if len(data.iloc[:,1])-data.iloc[:,a].isnull().sum() == 0:\n",
    "#         print data.columns[a], len(data.iloc[:,1])-data.iloc[:,a].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def day_to_event(data, test_data, date, test_date):\n",
    "    data[\"Days_to_Event\"] = (date - data['AddDate Date']).dt.days\n",
    "    test_data[\"Days_to_Event\"] = (test_date - test_data['AddDate Date']).dt.days\n",
    "    return data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def day_to_event_modify(data, test_data, date, test_date):\n",
    "    data[\"Days_to_Event_Modify\"] = (date - data['ModifyDate Date']).dt.days\n",
    "    test_data[\"Days_to_Event_Modify\"] = (test_date - test_data['ModifyDate Date']).dt.days\n",
    "    return data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def y_for_train(train_data):\n",
    "    y =[]\n",
    "    for a in train_data['FirstPrintDate Date']:\n",
    "        if a == 'nan':\n",
    "            y.append(0)\n",
    "        else:\n",
    "            y.append(1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def y_for_test(test_data):\n",
    "    y =[]\n",
    "    for a in test_data['FirstPrintDate Date']:\n",
    "        if a == 'nan':\n",
    "            y.append(0)\n",
    "        else:\n",
    "            y.append(1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_column_sync(data, test_data):\n",
    "    col=[]\n",
    "    for a in data.columns:\n",
    "        for b in test_data.columns:\n",
    "            if a == b:\n",
    "                col.append(b)    \n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_impute_Distance(data):\n",
    "    imp=Imputer(missing_values=\"NaN\", strategy=\"mean\" )\n",
    "    data[\"Distance\"]=imp.fit_transform(data[[\"Distance\"]]).ravel()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_impute(X):\n",
    "    data_X_hold = X.columns\n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=1)\n",
    "    imp.fit(X)\n",
    "    data_T=imp.transform(X)\n",
    "    X = pd.DataFrame(data_T, columns = data_X_hold)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_processing(data, test_data):\n",
    "    date = datetime.datetime(2016,7,23)\n",
    "    test_date = datetime.datetime(2017,7,13)\n",
    "    train_Y = y_for_train(data)\n",
    "    test_Y = y_for_test(test_data)\n",
    "    data = data_impute_Distance(data)\n",
    "    test_data = data_impute_Distance(test_data)\n",
    "    data, test_data = day_to_event(data, test_data, date, test_date)\n",
    "    data, test_data = day_to_event_modify(data, test_data, date, test_date)\n",
    "    test_data = test_data.apply(pd.to_numeric, errors='ignore')\n",
    "    train_X = data.select_dtypes(include = ['int64', 'float64', 'uint8', 'bool']).iloc[:, 0:]\n",
    "    test_X = test_data.select_dtypes(include = ['int64', 'float64', 'uint8', 'bool']).iloc[:, 0:]\n",
    "    col = train_test_column_sync(train_X, test_X)\n",
    "    train_X = data[col]\n",
    "    test_X = test_data[col]\n",
    "    train_X = data_impute(train_X)\n",
    "    test_X = data_impute(test_X)\n",
    "    return (train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = train_test_processing(data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X.loc[:,'sum_response'] = train_X.loc[:,'0201':'508'].sum(axis=1)\n",
    "test_X.loc[:,'sum_response'] = test_X.loc[:,'0201':'508'].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas_ml as pdml\n",
    "import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_Y = pd.DataFrame(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_normalized = pd.concat([train_X, train_Y], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_normalized = pdml.ModelFrame(data_normalized, target = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = data_normalized.imbalance.over_sampling.SMOTE()\n",
    "sampled = data_normalized.fit_sample(sampler)\n",
    "sampled=pd.DataFrame(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled = sampled.sample(frac=1)\n",
    "train_Y = sampled[0]\n",
    "train_X = sampled.drop([0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Distance', u'Phone1CountryPrefix', u'Phone2Number',\n",
       "       u'FaxCountryPrefix', u'AmountBilled', u'AmountPaid', u'AmountDue',\n",
       "       u'0201', u'0202', u'0203',\n",
       "       ...\n",
       "       u'MemberStatus_FullMember', u'MemberStatus_NonMember',\n",
       "       u'MemberStatus_nan', u'MemberStatus_nan.1', u'TermsAccepted_accepted',\n",
       "       u'TermsAccepted_nan', u'TermsAccepted_nan.1', u'Days_to_Event',\n",
       "       u'Days_to_Event_Modify', u'sum_response'],\n",
       "      dtype='object', length=304)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_T = train_X[['Days_to_Event','Days_to_Event_Modify','TermsAccepted_accepted','Email_Username','AmountBilled','PET','Distance','CA', 'NV', 'AZ','TX','WA','CO','sum_response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Days_to_Event', u'Days_to_Event_Modify', u'Email_Username', u'PET',\n",
       "       u'Distance', u'CA', u'NV', u'AZ', u'TX', u'WA', u'CO', u'sum_response'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_T.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "num_instances = len(train_T) \n",
    "seed = 7\n",
    "scoring = 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.748413 (0.008328)\n",
      "LASSO: 0.587483 (0.006108)\n",
      "Ridge: 0.723309 (0.008809)\n",
      "LDA: 0.723316 (0.008816)\n",
      "NB: 0.695429 (0.008781)\n",
      "KNeighborsClassifier: 0.745531 (0.007023)\n",
      "XGBClassifier: 0.813534 (0.008151)\n",
      "GradientBoostingClassifier: 0.817716 (0.008085)\n",
      "AdaBoostClassifier: 0.776325 (0.009434)\n",
      "RandomForestClassifier: 0.847637 (0.009854)\n",
      "ExtraTreesClassifier: 0.835038 (0.008808)\n",
      "DecisionTreeClassifier: 0.761498 (0.012823)\n"
     ]
    }
   ],
   "source": [
    "models=[]\n",
    "models.append(('LR', LogisticRegression(random_state = seed)))\n",
    "models.append(('LASSO', Lasso())) \n",
    "models.append(('Ridge', Ridge())) \n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('KNeighborsClassifier', KNeighborsClassifier()))#ewights = 'distance' \n",
    "models.append(('XGBClassifier', xgb.XGBClassifier()))\n",
    "models.append(('GradientBoostingClassifier', GradientBoostingClassifier(random_state = seed)))\n",
    "models.append(('AdaBoostClassifier', AdaBoostClassifier(random_state = seed)))\n",
    "models.append(('RandomForestClassifier', RandomForestClassifier(random_state = seed)))\n",
    "models.append(('ExtraTreesClassifier', ExtraTreesClassifier(random_state = seed)))\n",
    "models.append(('DecisionTreeClassifier', DecisionTreeClassifier(random_state = seed)))\n",
    "#models.append(('SVC', SVC(kernel = 'linear', random_state = seed)))\n",
    "#models.append(('LinearSVR', LinearSVR(random_state = seed)))\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(shuffle = True, n_splits=num_folds, random_state=seed)\n",
    "    \n",
    "    cv_results = model_selection.cross_val_score(model, train_T, train_Y, cv=kfold, scoring = scoring)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_P= test_X[['Days_to_Event','Days_to_Event_Modify','TermsAccepted_accepted','Email_Username','PET','Distance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(train_T, train_Y)\n",
    "#test_preds = model.predict(test_X)\n",
    "array = model.predict(train_T)\n",
    "probab = model.predict_proba(train_T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.647284929835\n",
      "0.724450384001\n"
     ]
    }
   ],
   "source": [
    "print metrics.accuracy_score(train_Y, array)\n",
    "print metrics.roc_auc_score(train_Y, probab[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probab[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the said array:\n",
      "[[   0    1]\n",
      " [8201 8189]]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(array, return_counts=True)  # this is predictions\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements, counts_elements = np.unique(train_Y, return_counts=True) #this is actual\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4328, 3867],\n",
       "       [1914, 6281]], dtype=int64)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train_Y, array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of feature: 16\n",
      "Feature Ranking: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "estimator = LogisticRegression(random_state = seed)\n",
    "rfe = RFECV(estimator,cv = kfold)\n",
    "fit = rfe.fit(train_T,train_Y)\n",
    "print(\"Num of feature: %d\") % fit.n_features_\n",
    "#print(\"Selected features: %s\") % fit.support_\n",
    "print(\"Feature Ranking: %s\") % fit.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_65_important_features = [] \n",
    "for b in range(0,len(fit.ranking_)):\n",
    "    if fit.ranking_[b] == 1:\n",
    "        top_65_important_features.append(b)\n",
    "        print b,train_X.columns[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_X = train_X.iloc[:,top_65_important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_65_important_features = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-2ef97b2fde34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%s: %f (%f)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "models=[]\n",
    "models.append(('LR', LogisticRegression(random_state = seed)))\n",
    "#models.append(('LASSO', Lasso())) \n",
    "#models.append(('Ridge', Ridge())) \n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('KNeighborsClassifier', KNeighborsClassifier())) \n",
    "models.append(('XGBClassifier', xgb.XGBClassifier()))\n",
    "models.append(('GradientBoostingClassifier', GradientBoostingClassifier(random_state = seed)))\n",
    "models.append(('AdaBoostClassifier', AdaBoostClassifier(random_state = seed)))\n",
    "models.append(('RandomForestClassifier', RandomForestClassifier(random_state = seed)))\n",
    "models.append(('ExtraTreesClassifier', ExtraTreesClassifier(random_state = seed)))\n",
    "models.append(('DecisionTreeClassifier', DecisionTreeClassifier(random_state = seed)))\n",
    "#models.append(('SVC', SVC(kernel = 'linear', random_state = seed)))\n",
    "#models.append(('LinearSVR', LinearSVR(random_state = seed)))\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(shuffle = True, n_splits=num_folds, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X, train_Y, cv=kfold, scoring= scoring)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame({'y': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z=[]\n",
    "Z=pd.concat([X,train_Y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = Z.select_dtypes(include = ['int64', 'float64','uint8']).iloc[:, 1:].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "plt.figure(figsize=(60, 60))\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(corr, mask=mask, vmin = -1, vmax=1, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LR', 'KNeighborsClassifier', 'RandomForestClassifier', 0.85369773310560304, 0.0074974399523090171)\n",
      "('LR', 'KNeighborsClassifier', 'ExtraTreesClassifier', 0.85493828459381971, 0.0087903236616036669)\n",
      "('LR', 'XGBClassifier', 'RandomForestClassifier', 0.85486887655878885, 0.0084441092122449091)\n",
      "('LR', 'XGBClassifier', 'ExtraTreesClassifier', 0.85615346320586616, 0.0087860251358491352)\n",
      "('LR', 'GradientBoostingClassifier', 'RandomForestClassifier', 0.85548179096871357, 0.0087556884215473311)\n",
      "('LR', 'GradientBoostingClassifier', 'ExtraTreesClassifier', 0.85671676466978164, 0.0090803949059594762)\n",
      "('LR', 'AdaBoostClassifier', 'RandomForestClassifier', 0.85188208214529548, 0.0088626699608252077)\n",
      "('LR', 'RandomForestClassifier', 'ExtraTreesClassifier', 0.85688839965691055, 0.0093673845581198571)\n",
      "('LDA', 'XGBClassifier', 'ExtraTreesClassifier', 0.85002648382492585, 0.0097600352542832527)\n",
      "('LDA', 'GradientBoostingClassifier', 'ExtraTreesClassifier', 0.85067654632557921, 0.0099652017836868688)\n",
      "('LDA', 'RandomForestClassifier', 'ExtraTreesClassifier', 0.85099909875708024, 0.010473968278613079)\n",
      "('KNeighborsClassifier', 'XGBClassifier', 'RandomForestClassifier', 0.85778940035757179, 0.0077747296865422304)\n",
      "('KNeighborsClassifier', 'XGBClassifier', 'ExtraTreesClassifier', 0.85922477764836136, 0.0086201635223766924)\n",
      "('KNeighborsClassifier', 'GradientBoostingClassifier', 'RandomForestClassifier', 0.85813013226815271, 0.0080148067711379514)\n",
      "('KNeighborsClassifier', 'GradientBoostingClassifier', 'ExtraTreesClassifier', 0.85954813064705182, 0.0087746308721897773)\n",
      "('KNeighborsClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier', 0.85105187841114682, 0.010110147404129272)\n",
      "('XGBClassifier', 'GradientBoostingClassifier', 'RandomForestClassifier', 0.8527855044722743, 0.0096591944054838889)\n",
      "('XGBClassifier', 'GradientBoostingClassifier', 'ExtraTreesClassifier', 0.85465811304155237, 0.0096226611451066483)\n",
      "('XGBClassifier', 'AdaBoostClassifier', 'RandomForestClassifier', 0.85422688700698879, 0.009878735345783549)\n",
      "('XGBClassifier', 'AdaBoostClassifier', 'ExtraTreesClassifier', 0.85391765351827775, 0.0099406959111860826)\n",
      "('XGBClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier', 0.85767957009322582, 0.0099945499973182108)\n",
      "('GradientBoostingClassifier', 'AdaBoostClassifier', 'RandomForestClassifier', 0.85498793192400435, 0.010080002000850567)\n",
      "('GradientBoostingClassifier', 'AdaBoostClassifier', 'ExtraTreesClassifier', 0.8545833276431527, 0.010016738518716902)\n",
      "('GradientBoostingClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier', 0.8581001058454909, 0.010051865041365727)\n"
     ]
    }
   ],
   "source": [
    "for a in range(0, len(models)):\n",
    "    model1 = models[a]\n",
    "    for b in range(a+1, len(models)):\n",
    "        model2 = models[b]\n",
    "        for c in range(b+1, len(models)):\n",
    "            model3 = models[c]\n",
    "            estimators = []\n",
    "            estimators.append(model1)\n",
    "            estimators.append(model2)\n",
    "            estimators.append(model3)\n",
    "            ensemble = VotingClassifier(estimators, voting='soft')\n",
    "            results = model_selection.cross_val_score(ensemble, train_T, train_Y, cv=kfold, scoring= scoring)\n",
    "            if results.mean() > .85:\n",
    "                print(model1[0], model2[0], model3[0],results.mean(), results.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "\n",
    "estimators.append(('LR',LogisticRegression(random_state = seed)))\n",
    "estimators.append(('XGBClassifier',xgb.XGBClassifier()))\n",
    "estimators.append(('KNeighborsClassifier', KNeighborsClassifier()))\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "ensemble.fit(train_T, train_Y)\n",
    "predict = ensemble.predict(test_P)\n",
    "probab = ensemble.predict_proba(test_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_selection.cross_val_score(ensemble, train_T, train_Y, cv=kfold, scoring= scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = ('LR', LogisticRegression(random_state = seed)'LR', 'XGBClassifier', 'RandomForestClassifier', 0.85486887655878885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RandomForestClassifier(random_state = seed)))\n",
    "models.append(('ExtraTreesClassifier', ExtraTreesClassifier(random_state = seed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_P = test_X[['Days_to_Event','Days_to_Event_Modify','TermsAccepted_accepted','Email_Username','AmountBilled','PET','Distance','CA', 'NV', 'AZ','TX','WA','CO','sum_response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state = seed)\n",
    "model.fit(train_T,train_Y)\n",
    "predict = model.predict(test_P)\n",
    "probab = model.predict_proba(test_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744288224956\n",
      "0.737400592742\n"
     ]
    }
   ],
   "source": [
    "print metrics.accuracy_score(test_Y, predict)\n",
    "print metrics.roc_auc_score(test_Y, probab[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1408, 1708],\n",
       "       [ 911, 6215]], dtype=int64)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_Y, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "e = 0\n",
    "for idx, val in enumerate(probab):\n",
    "    if val[0] > .80:\n",
    "        c += 1\n",
    "        if test_Y[idx] == 0:\n",
    "            print c, 0, idx, val[0]\n",
    "        else: \n",
    "            e += 1\n",
    "            print c, 0, idx, val[0], \"error\"\n",
    "            \n",
    "   \n",
    "    if val[1] > .80:\n",
    "        c += 1\n",
    "        if test_Y[idx] == 1:\n",
    "            print c, 1, idx, val[1]\n",
    "        else:\n",
    "            e += 1\n",
    "            print c, 1, idx, val[1], \"error\"\n",
    "print e\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = []\n",
    "estimators.append(('AdaBoostClassifier', AdaBoostClassifier(random_state = seed)))\n",
    "estimators.append(('GradientBoostingClassifier', GradientBoostingClassifier(random_state = seed)))\n",
    "estimators.append(('ExtraTreesClassifier', ExtraTreesClassifier(random_state = seed)))\n",
    "ensemble = VotingClassifier(estimators, voting='soft')\n",
    "ensemble.fit(X, train_Y)\n",
    "predict = ensemble.predict(test_X[X.columns])\n",
    "probab = ensemble.predict_proba(test_X[X.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print metrics.accuracy_score(test_Y, predict)\n",
    "print metrics.roc_auc_score(test_Y, probab[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print metrics.confusion_matrix(test_Y, predict)\n",
    "#print metrics.classification_report(test_Y, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
